{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUSDB18 Dataset Preparation for Source Separation\n",
    "\n",
    "This notebook:\n",
    "1. Loads MUSDB18 dataset\n",
    "2. Converts stereo to mono\n",
    "3. Normalizes audio\n",
    "4. Extracts 3-second segments\n",
    "5. Calculates spectrograms\n",
    "6. Saves as PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f718403bd30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import musdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "SAMPLE_RATE = 44100\n",
    "SEGMENT_DURATION = 0.9  # seconds\n",
    "SEGMENT_SAMPLES = int(SAMPLE_RATE * SEGMENT_DURATION)\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "SAVE_DIR = '../data/processed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_musdb(subset='train'):\n",
    "    \"\"\"Load MUSDB18 dataset.\"\"\"\n",
    "    \n",
    "    return musdb.DB(root=\"../data/raw/\", subsets=[subset])[:2]\n",
    "\n",
    "def stereo_to_mono(audio):\n",
    "    \"\"\"Convert stereo audio to mono by averaging channels.\"\"\"\n",
    "    return np.mean(audio, axis=1) if audio.ndim > 1 else audio\n",
    "\n",
    "def normalize_audio(audio):\n",
    "    \"\"\"Normalize audio to [-1, 1] range.\"\"\"\n",
    "    return audio / np.max(np.abs(audio))\n",
    "\n",
    "def extract_segments(waveform, sample_rate=44100, segment_duration=3):\n",
    "    \"\"\"\n",
    "    Extrait des segments de durée fixe d'une forme d'onde.\n",
    "    \n",
    "    Arguments:\n",
    "        waveform (np.array): La forme d'onde à segmenter (1D array)\n",
    "        sample_rate (int): Fréquence d'échantillonnage en Hz (par défaut: 44100 Hz)\n",
    "        segment_duration (float): Durée des segments en secondes (par défaut: 3 secondes)\n",
    "    \n",
    "    Retourne:\n",
    "        np.array: Un tableau 2D de taille (n_segments, n_echantillons) contenant les segments\n",
    "    \"\"\"\n",
    "    # Calcul du nombre d'échantillons par segment\n",
    "    samples_per_segment = int(sample_rate * segment_duration)\n",
    "    \n",
    "    # Calcul du nombre total de segments\n",
    "    total_samples = len(waveform)\n",
    "    n_segments = (total_samples + samples_per_segment - 1) // samples_per_segment\n",
    "    \n",
    "    # Création d'un tableau pour stocker les segments\n",
    "    segments = np.zeros((n_segments, samples_per_segment))\n",
    "    \n",
    "    # Extraction des segments\n",
    "    for i in range(n_segments):\n",
    "        start_idx = i * samples_per_segment\n",
    "        end_idx = min(start_idx + samples_per_segment, total_samples)\n",
    "        \n",
    "        # Copie des échantillons dans le segment\n",
    "        segment_length = end_idx - start_idx\n",
    "        segments[i, :segment_length] = waveform[start_idx:end_idx]\n",
    "        \n",
    "    return segments\n",
    "\n",
    "def compute_spectrogram(audio):\n",
    "    \"\"\"Compute magnitude spectrogram using PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        audio: 1D numpy array or tensor of shape (segment_samples,)\n",
    "              where segment_samples = SAMPLE_RATE * SEGMENT_DURATION\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Magnitude spectrogram of shape (n_fft//2 + 1, n_frames) where n_frames = (segment_samples - n_fft) // hop_length + 1\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if not already\n",
    "    if not isinstance(audio, torch.Tensor):\n",
    "        audio = torch.FloatTensor(audio).to('cuda')\n",
    "    \n",
    "    # Compute STFT\n",
    "    stft = torch.stft(\n",
    "        audio,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        return_complex=True\n",
    "    )\n",
    "    \n",
    "    # Convert to magnitude spectrogram\n",
    "    magnitudes = torch.abs(stft)\n",
    "    \n",
    "    return magnitudes.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(subset='train'):\n",
    "    \"\"\"Process the dataset and return spectrograms for mixture and sources.\"\"\"\n",
    "    mus = load_musdb(subset)\n",
    "    all_mix_specs = []  # List to store all segment spectrograms\n",
    "    all_source_specs = []  # List to store all source spectrograms\n",
    "    \n",
    "    print(f\"Found {len(mus)} tracks in {subset} set\")\n",
    "    \n",
    "    # Main progress bar for tracks\n",
    "    pbar_tracks = tqdm(mus, desc=f\"Processing {subset} tracks\", unit=\"track\")\n",
    "    \n",
    "    for track_idx, track in enumerate(pbar_tracks):\n",
    "        # Update progress bar description with current track\n",
    "        pbar_tracks.set_description(f\"Processing {track.name}\")\n",
    "        \n",
    "        # Process mixture\n",
    "        track.audio = track.audio.T\n",
    "        mix_audio = stereo_to_mono(track.audio.T)\n",
    "        mix_audio = normalize_audio(mix_audio)\n",
    "        mix_segments = extract_segments(mix_audio, SEGMENT_SAMPLES)\n",
    "        print(\"mix_segments :\", mix_segments.shape)\n",
    "        \n",
    "        # Process sources (vocals, drums, bass, other)\n",
    "        sources_audio = {}\n",
    "        \n",
    "        # Process sources first\n",
    "        for source in tqdm(['vocals', 'drums', 'bass', 'other'], desc=\"Processing sources\", leave=False):\n",
    "            source_audio = stereo_to_mono(track.targets[source].audio.T)\n",
    "            source_audio = normalize_audio(source_audio)\n",
    "            sources_audio[source] = extract_segments(source_audio, SEGMENT_SAMPLES)\n",
    "        \n",
    "        # Compute spectrograms for each segment\n",
    "        for i in tqdm(range(len(mix_segments)), desc=\"Computing spectrograms\", leave=False):\n",
    "            # Process mixture spectrogram\n",
    "            mix_spec = compute_spectrogram(mix_segments[i])\n",
    "            all_mix_specs.append(mix_spec)\n",
    "            print(len(all_mix_specs))\n",
    "            \n",
    "            # Process source spectrograms\n",
    "            segment_sources = []\n",
    "            for source in ['vocals', 'drums', 'bass', 'other']:\n",
    "                source_spec = compute_spectrogram(sources_audio[source][i])\n",
    "                segment_sources.append(source_spec)\n",
    "            \n",
    "            all_source_specs.append(np.stack(segment_sources))\n",
    "\n",
    "    \n",
    "    # Convert to PyTorch tensors with proper dimensions\n",
    "    # Shape: (n_segments, freq_bins, time_frames)\n",
    "    X = torch.FloatTensor(np.stack(all_mix_specs))\n",
    "    # Shape: (n_segments, n_sources, freq_bins, time_frames)\n",
    "    y = torch.FloatTensor(np.stack(all_source_specs))\n",
    "    \n",
    "    # Add a channel dimension for the mixture\n",
    "    X = X.unsqueeze(1)  # Shape: (n_segments, 1, freq_bins, time_frames)\n",
    "    \n",
    "    print(f\"\\nFinal dataset shapes:\")\n",
    "    print(f\"X: {X.shape}\")\n",
    "    print(f\"y: {y.shape}\")\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Found 2 tracks in train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing A Classic Education - NightOwl:   0%|          | 0/2 [00:00<?, ?track/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mix_segments : (64, 119070)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing A Classic Education - NightOwl:   0%|          | 0/2 [00:07<?, ?track/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1025, 233])\n",
      "1\n",
      "torch.Size([1025, 233])\n",
      "torch.Size([1025, 233])\n",
      "torch.Size([1025, 233])\n",
      "torch.Size([1025, 233])\n",
      "torch.Size([1025, 233])\n",
      "2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Process training data\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing training data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m X_train, y_train = \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m# Process test data\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03mprint(\"Processing test data...\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m \u001b[33;03mprint(f\"y_test: {y_test.shape}\")\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mprocess_dataset\u001b[39m\u001b[34m(subset)\u001b[39m\n\u001b[32m     40\u001b[39m segment_sources = []\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mvocals\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdrums\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbass\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mother\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     source_spec = compute_spectrogram(\u001b[43msources_audio\u001b[49m\u001b[43m[\u001b[49m\u001b[43msource\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     43\u001b[39m     segment_sources.append(source_spec)\n\u001b[32m     45\u001b[39m all_source_specs.append(np.stack(segment_sources))\n",
      "\u001b[31mIndexError\u001b[39m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "X_train, y_train = process_dataset('train')\n",
    "\n",
    "\"\"\"\n",
    "# Process test data\n",
    "print(\"Processing test data...\")\n",
    "X_test, y_test = process_dataset('test')\n",
    "\n",
    "# Save processed data\n",
    "torch.save({\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test\n",
    "}, os.path.join(SAVE_DIR, 'processed_musdb18.pt'))\n",
    "\n",
    "print(f\"Saved processed data with shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
