{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd00b16-01bb-4aa7-9c84-ebe8cb89bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68eabc70-60c0-4610-a8f8-190c403d4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramSeparator(nn.Module):\n",
    "    def __init__(self, n_mels=128, seq_len=800, n_sources=4, d_model=256, nhead=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.n_sources = n_sources\n",
    "        self.seq_len = seq_len\n",
    "        self.n_mels = n_mels\n",
    "        \n",
    "        # Convolutional encoder\n",
    "        self.conv_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Projection to d_model dimension\n",
    "        self.flatten_proj = nn.Linear(128 * n_mels, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=d_model*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Transformer decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Source-specific queries (learnable)\n",
    "        self.source_queries = nn.Parameter(torch.randn(n_sources, seq_len, d_model))\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(d_model, n_mels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 128, 800)\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Add channel dimension and apply CNN\n",
    "        x = x.unsqueeze(1)  # (B, 1, 128, 800)\n",
    "        x = self.conv_encoder(x)  # (B, 128, 128, 800)\n",
    "        \n",
    "        # Prepare for transformer\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, T, C, M)\n",
    "        B, T, C, M = x.shape\n",
    "        x = x.reshape(B, T, C * M)  # (B, T, C*M)\n",
    "        x = self.flatten_proj(x)  # (B, T, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoding[:T]\n",
    "        \n",
    "        # Transformer encoder\n",
    "        memory = self.transformer_encoder(x)  # (B, T, d_model)\n",
    "        \n",
    "        # Prepare source queries\n",
    "        queries = self.source_queries.expand(B, -1, -1, -1)  # (B, S, T, d_model)\n",
    "        S = queries.shape[1]\n",
    "        queries = queries.reshape(B*S, T, -1)  # (B*S, T, d_model)\n",
    "        \n",
    "        # Expand memory for each source\n",
    "        memory = memory.unsqueeze(1).expand(-1, S, -1, -1)  # (B, S, T, d_model)\n",
    "        memory = memory.reshape(B*S, T, -1)  # (B*S, T, d_model)\n",
    "        \n",
    "        # Transformer decoder\n",
    "        output = self.transformer_decoder(queries, memory)  # (B*S, T, d_model)\n",
    "        \n",
    "        # Project to mel spectrum\n",
    "        output = self.output_proj(output)  # (B*S, T, n_mels)\n",
    "        \n",
    "        # Reshape to final format\n",
    "        output = output.reshape(B, S, T, self.n_mels)  # (B, S, T, n_mels)\n",
    "        output = output.permute(0, 1, 3, 2)  # (B, S, n_mels, T)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e664de45-f0af-4359-9878-7c6f78d3d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedSpectrogramDataset(Dataset):\n",
    "    def __init__(self, X_path, y_path, device='cpu', preload=True, pin_memory=False):\n",
    "        \"\"\"\n",
    "        Dataset optimisé pour charger complètement les données en mémoire\n",
    "        \n",
    "        Args:\n",
    "            X_path (str): Chemin vers le fichier .npy des mélanges\n",
    "            y_path (str): Chemin vers le fichier .npy des sources\n",
    "            device (str): Dispositif où stocker les données ('cpu' ou 'cuda')\n",
    "            preload (bool): Si True, précharge toutes les données en mémoire\n",
    "            pin_memory (bool): Si True, utilise torch.pin_memory() pour accélérer le transfert CPU->GPU\n",
    "        \"\"\"\n",
    "        self.X_path = X_path\n",
    "        self.y_path = y_path\n",
    "        self.device = device\n",
    "        self.X_data = None\n",
    "        self.y_data = None\n",
    "        self.pin_memory = pin_memory and device == 'cpu'  # pin_memory n'est utile que sur CPU\n",
    "        \n",
    "        # Préchargement des données\n",
    "        if preload:\n",
    "            self._preload_data()\n",
    "    \n",
    "    def _preload_data(self):\n",
    "        \"\"\"Précharge toutes les données en mémoire\"\"\"\n",
    "        print(f\"Préchargement des données depuis {self.X_path} et {self.y_path}...\")\n",
    "        \n",
    "        # Charger les données\n",
    "        X_data = np.load(self.X_path)\n",
    "        y_data = np.load(self.y_path)\n",
    "        \n",
    "        # Convertir en tenseurs PyTorch\n",
    "        self.X_data = torch.tensor(X_data, dtype=torch.float32, device=self.device)\n",
    "        self.y_data = torch.tensor(y_data, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        if self.pin_memory and self.device == 'cpu':\n",
    "            self.X_data = self.X_data.pin_memory()\n",
    "            self.y_data = self.y_data.pin_memory()\n",
    "        \n",
    "        print(f\"Données chargées en mémoire: X={self.X_data.shape}, y={self.y_data.shape}\")\n",
    "        \n",
    "        # Libérer la mémoire numpy\n",
    "        del X_data, y_data\n",
    "        gc.collect()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.X_data is not None:\n",
    "            return len(self.X_data)\n",
    "        else:\n",
    "            # Déterminer la taille sans charger toutes les données\n",
    "            return len(np.load(self.X_path, mmap_mode='r'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.X_data is not None:\n",
    "            # Récupérer depuis le cache préchargé\n",
    "            return self.X_data[idx], self.y_data[idx]\n",
    "        else:\n",
    "            # Chargement à la volée (plus lent)\n",
    "            X = np.load(self.X_path, mmap_mode='r')[idx]\n",
    "            y = np.load(self.y_path, mmap_mode='r')[idx]\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "            return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef7cc2d-7353-48a5-af25-6c9bccfd0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SI_SDR_Loss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: tensor of shape [B, S, F, T] (batch, sources, freq_bins, time)\n",
    "            targets: tensor of shape [B, S, F, T]\n",
    "        Returns:\n",
    "            SI-SDR loss (negative SI-SDR for minimization)\n",
    "        \"\"\"\n",
    "        # Reshape to [B*S, F*T]\n",
    "        B, S, F, T = predictions.shape\n",
    "        predictions = predictions.reshape(B*S, -1)\n",
    "        targets = targets.reshape(B*S, -1)\n",
    "\n",
    "        # Zero-mean normalization\n",
    "        predictions = predictions - torch.mean(predictions, dim=-1, keepdim=True)\n",
    "        targets = targets - torch.mean(targets, dim=-1, keepdim=True)\n",
    "\n",
    "        # Calculate SI-SDR\n",
    "        alpha = (torch.sum(predictions * targets, dim=-1, keepdim=True) + self.eps) / (\n",
    "            torch.sum(targets ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        scaled_target = alpha * targets\n",
    "\n",
    "        si_sdr = torch.sum(scaled_target ** 2, dim=-1) / (\n",
    "            torch.sum((predictions - scaled_target) ** 2, dim=-1) + self.eps)\n",
    "        si_sdr = 10 * torch.log10(si_sdr + self.eps)\n",
    "\n",
    "        # Return negative mean for loss minimization\n",
    "        return -si_sdr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8976568-bf57-4b0e-a16c-38d18024a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir, results_dir, batch_size=8, num_epochs=4, \n",
    "                learning_rate=1e-5, gradient_accumulation_steps=8,\n",
    "                preload_data=True, use_amp=True):\n",
    "    \"\"\"\n",
    "    Fonction d'entraînement optimisée avec:\n",
    "    - Préchargement des données\n",
    "    - Mixed Precision Training (AMP)\n",
    "    - Gradient Accumulation\n",
    "    - Nettoyage de la mémoire GPU\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Répertoire contenant les données\n",
    "        results_dir: Répertoire pour sauvegarder les modèles\n",
    "        batch_size: Taille des batchs\n",
    "        num_epochs: Nombre d'époques d'entraînement\n",
    "        learning_rate: Taux d'apprentissage\n",
    "        gradient_accumulation_steps: Nombre de pas pour l'accumulation de gradient\n",
    "        preload_data: Si True, précharge les données en mémoire\n",
    "        use_amp: Si True, utiliser le training en précision mixte\n",
    "    \"\"\"\n",
    "    # Créer le répertoire de résultats s'il n'existe pas\n",
    "    os.makedirs(os.path.join(results_dir, \"models\"), exist_ok=True)\n",
    "    \n",
    "    # Détection du dispositif\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Utilisation du dispositif: {device}\")\n",
    "    \n",
    "    # Chemins des données\n",
    "    X_train_path = os.path.join(data_dir, \"processed/X_train.npy\")\n",
    "    y_train_path = os.path.join(data_dir, \"processed/y_train.npy\")\n",
    "    X_test_path = os.path.join(data_dir, \"processed/X_test.npy\")\n",
    "    y_test_path = os.path.join(data_dir, \"processed/y_test.npy\")\n",
    "    \n",
    "    # Datasets optimisés\n",
    "    train_dataset = OptimizedSpectrogramDataset(\n",
    "        X_train_path, y_train_path, \n",
    "        device='cpu',  # Toujours garder les données sur CPU pour éviter la saturation de la VRAM\n",
    "        preload=preload_data,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = OptimizedSpectrogramDataset(\n",
    "        X_test_path, y_test_path, \n",
    "        device='cpu',\n",
    "        preload=preload_data,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # DataLoaders optimisés\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=4,  # Utiliser plusieurs workers pour le chargement parallèle\n",
    "        pin_memory=True  # Transfert CPU->GPU plus rapide\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialiser le modèle, le critère et l'optimiseur\n",
    "    model = SpectrogramSeparator().to(device)\n",
    "    criterion = SI_SDR_Loss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Activer le gradient checkpointing si disponible (économise de la mémoire)\n",
    "    if hasattr(model, 'transformer_encoder') and hasattr(model.transformer_encoder, 'gradient_checkpointing_enable'):\n",
    "        model.transformer_encoder.gradient_checkpointing_enable()\n",
    "        print(\"Gradient checkpointing activé pour l'encodeur\")\n",
    "    \n",
    "    if hasattr(model, 'transformer_decoder') and hasattr(model.transformer_decoder, 'gradient_checkpointing_enable'):\n",
    "        model.transformer_decoder.gradient_checkpointing_enable()\n",
    "        print(\"Gradient checkpointing activé pour le décodeur\")\n",
    "    \n",
    "    # Initialiser le scaler pour la précision mixte (AMP)\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        optimizer.zero_grad()  # Réinitialiser une fois par époque\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        \n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(pbar):\n",
    "            # Transférer les données sur GPU\n",
    "            X_batch = X_batch.to(device, non_blocking=True)\n",
    "            y_batch = y_batch.to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass avec AMP si activé\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(X_batch)\n",
    "                    loss = criterion(output, y_batch) / gradient_accumulation_steps\n",
    "                \n",
    "                # Backward pass avec scaling\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Mise à jour des poids tous les n batchs\n",
    "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                # Mode normal (sans AMP)\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output, y_batch) / gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                \n",
    "                # Mise à jour des poids tous les n batchs\n",
    "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "            # Multiplier par gradient_accumulation_steps pour obtenir la vraie valeur\n",
    "            batch_loss = loss.item() * gradient_accumulation_steps\n",
    "            epoch_loss += batch_loss\n",
    "            pbar.set_postfix({\"SI-SDR Loss\": f\"{batch_loss:.4f}\"})\n",
    "            \n",
    "            # Libérer la mémoire GPU\n",
    "            del X_batch, y_batch, output\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Vérifier s'il reste des gradients à appliquer\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps != 0:\n",
    "            if use_amp:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Évaluation sur le set de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                X_test = X_test.to(device, non_blocking=True)\n",
    "                y_test = y_test.to(device, non_blocking=True)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        output = model(X_test)\n",
    "                        loss = criterion(output, y_test)\n",
    "                else:\n",
    "                    output = model(X_test)\n",
    "                    loss = criterion(output, y_test)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                # Libérer la mémoire\n",
    "                del X_test, y_test, output\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "        # Sauvegarder le meilleur modèle\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, os.path.join(results_dir, \"models/model.pth\"))\n",
    "            print(f\"Meilleur modèle sauvegardé avec loss = {best_loss:.4f}\")\n",
    "        \n",
    "        # Libérer la mémoire à la fin de chaque époque\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2209cebe-19af-4ee0-bf1a-d89d66ae7aa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du dispositif: cuda\n",
      "Préchargement des données depuis ../data/processed/X_train.npy et ../data/processed/y_train.npy...\n",
      "Données chargées en mémoire: X=torch.Size([2410, 128, 800]), y=torch.Size([2410, 4, 128, 800])\n",
      "Préchargement des données depuis ../data/processed/X_test.npy et ../data/processed/y_test.npy...\n",
      "Données chargées en mémoire: X=torch.Size([1319, 128, 800]), y=torch.Size([1319, 4, 128, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8: 100%|███████████████████████████| 2410/2410 [04:45<00:00,  8.45batch/s, SI-SDR Loss=4.1434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Loss: 12.5541\n",
      "Test Loss: 12.5989\n",
      "Meilleur modèle sauvegardé avec loss = 12.5989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8: 100%|███████████████████████████| 2410/2410 [04:49<00:00,  8.33batch/s, SI-SDR Loss=3.9988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8, Loss: 9.9905\n",
      "Test Loss: 11.1985\n",
      "Meilleur modèle sauvegardé avec loss = 11.1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8: 100%|███████████████████████████| 2410/2410 [04:47<00:00,  8.38batch/s, SI-SDR Loss=6.8206]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8, Loss: 8.8554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 10.8709\n",
      "Meilleur modèle sauvegardé avec loss = 10.8709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8: 100%|███████████████████████████| 2410/2410 [04:33<00:00,  8.82batch/s, SI-SDR Loss=7.5879]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8, Loss: 8.6203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 10.8625\n",
      "Meilleur modèle sauvegardé avec loss = 10.8625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8: 100%|███████████████████████████| 2410/2410 [04:24<00:00,  9.12batch/s, SI-SDR Loss=2.6044]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8, Loss: 8.5416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 10.8344\n",
      "Meilleur modèle sauvegardé avec loss = 10.8344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8: 100%|███████████████████████████| 2410/2410 [04:40<00:00,  8.58batch/s, SI-SDR Loss=5.8152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8, Loss: 8.6747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 13.4472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8: 100%|███████████████████████████| 2410/2410 [04:42<00:00,  8.53batch/s, SI-SDR Loss=2.5275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8, Loss: 8.1957\n",
      "Test Loss: 10.1391\n",
      "Meilleur modèle sauvegardé avec loss = 10.1391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8: 100%|███████████████████████████| 2410/2410 [04:39<00:00,  8.61batch/s, SI-SDR Loss=3.4511]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8, Loss: 7.7623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 10.0299\n",
      "Meilleur modèle sauvegardé avec loss = 10.0299\n"
     ]
    }
   ],
   "source": [
    "# Paramètres d'entraînement\n",
    "DATA_DIR = \"../data\"\n",
    "RESULTS_DIR = \"../results\"\n",
    "BATCH_SIZE = 1  # Réduit pour économiser de la mémoire\n",
    "NUM_EPOCHS = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Équivaut à un batch_size effectif de 4\n",
    "USE_AMP = False  # Utiliser la précision mixte\n",
    "\n",
    "# Lancer l'entraînement\n",
    "train_model(\n",
    "    data_dir=DATA_DIR,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    preload_data=True, \n",
    "    use_amp=USE_AMP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc84e8-a860-436d-afa9-5565438a8f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
